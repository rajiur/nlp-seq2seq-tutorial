{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: center; font-size: 36px;\"> <b> COSC6336: Homework-3 Report </b></div>\n",
    "<div style=\"text-align: center; font-size: small;\">submitted by</div>\n",
    "<div style=\"text-align: center; font-size: large;\">Mohammad Rajiur Rahman</div>\n",
    "<div style=\"text-align: center; font-size: small;\">UHID: XXXXXXX </div>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim of the work is to learn sequence to sequence (seq2seq) models. Introduced for the first time in 2014 by Google, a sequence to sequence model aims to map a fixed-length input with a fixed-length output where the length of the input and output may differ. From the high-level view, a seq2seq model has encoder, decoder and intermediate step as its main components. The class tutorial shows an excellent example of a minimalistic implementation of seq2seq model. The goal of the homework is to make this model efficient using batches and analyze the models' performance.\n",
    "\n",
    "The overall architechture of the class tutorial was kept which is uses a single LSTM layer with one direction on both the encoder and decoder. Two models were designed. One with attention and another without attention. Both models works with batches of input. <b>The model with attention achieved 98.80% accuracy on test test.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The task uses a toy dataset from automatically-generated data. It has 20000 records for training and 5000 for validation. The test dataset of 2000 records are fixed (provided in a text file: <a href='test.txt'>test.txt</a>). The test dataset is different in terms of length of input and output compared to training and validation dataset\n",
    "\n",
    "|       \t| Train Input \t| Train Output \t| Validation Input \t| Validation Ouptut \t| Test Input \t| Test output \t|\n",
    "|:-----:\t|------------:\t|-------------:\t|-----------------:\t|------------------:\t|-----------:\t|------------:\t|\n",
    "| count \t|       20000 \t|        20000 \t|             5000 \t|              5000 \t|       5000 \t|        5000 \t|\n",
    "| mean  \t|       13.02 \t|         5.77 \t|            12.92 \t|              5.72 \t|      23.80 \t|        6.82 \t|\n",
    "| std   \t|        5.04 \t|         1.94 \t|             5.03 \t|              1.96 \t|      10.27 \t|        2.51 \t|\n",
    "| min   \t|           3 \t|            1 \t|                3 \t|                 1 \t|          3 \t|           2 \t|\n",
    "| 25%   \t|           9 \t|            4 \t|                9 \t|                 4 \t|         15 \t|           5 \t|\n",
    "| 50%   \t|          13 \t|            6 \t|               13 \t|                 6 \t|         23 \t|           7 \t|\n",
    "| 75%   \t|          17 \t|            7 \t|               17 \t|                 7 \t|         32 \t|           9 \t|\n",
    "| max   \t|          28 \t|           10 \t|               28 \t|                10 \t|         58 \t|          13 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRMEFVeP6FsX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from os import path\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up environment (cuda, if available otherwise cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "#uncomment following line if cuda is available\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CznvYR656Fsb"
   },
   "outputs": [],
   "source": [
    "# nothing changed from baseline code\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(20_000)\n",
    "valid_inp, valid_out = sorting_letters_dataset(5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SM_M8mL46Fsf"
   },
   "outputs": [],
   "source": [
    "# nothing changed from baseline code\n",
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "    \n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<stop>','<start>'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']\n",
    "PAD_IX = tgt_vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TC-k7j8A6Fsj"
   },
   "source": [
    "**Process Data**\n",
    "\n",
    "Converting data to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5e4pB4oA6Fsk"
   },
   "outputs": [],
   "source": [
    "# minor chages from baseline code\n",
    "def map_elems(elems, mapper, istarget=False):\n",
    "    if istarget: # add stop if target\n",
    "         return [mapper[elem] for elem in elems]+[STOP_IX]\n",
    "    else: \n",
    "         return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper, istarget):\n",
    "    return [map_elems(elems, mapper, istarget) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi, istarget=False)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi, istarget=True) # add stop token if target\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi, istarget=False)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi, istarget=True) # add stop token if target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batching Process** \n",
    "\n",
    "Functions to Create Batch of Data with Padding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad a sequence with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    result = [] + seq\n",
    "    result += [PAD_IX for i in range(max_length - len(seq))]\n",
    "    return result\n",
    "\n",
    "# Pad batch of sequesnces\n",
    "def pad_batch(seqs, max_length):\n",
    "    return [pad_seq(x, max_length) for x in seqs]\n",
    "\n",
    "# process batch: sort them by length of sequences, add stopad them \n",
    "def process_batch(input_seqs, target_seqs):\n",
    "    \n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs] # max_seq_len_input = max(input_lengths)\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs] # max_seq_len_target = max(input_lengths)\n",
    "    \n",
    "    # Turn padded arrays into tensors \n",
    "    input_var = (torch.LongTensor(input_padded)) # (batch_size x max_seq_len_input) \n",
    "    target_var = (torch.LongTensor(target_padded)).transpose(0, 1) #transpose target into (max_seq_len_target x batch_size)\n",
    "    input_lengths = (torch.LongTensor(input_lengths)) # batch_size\n",
    "    target_lengths = (torch.LongTensor(target_lengths)) # batch_size\n",
    "    \n",
    "    input_var = input_var.to(device) # move to device\n",
    "    target_var = target_var.to(device) # move to device\n",
    "    \n",
    "    return input_var, input_lengths, target_var, target_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "The batched sequence to sequence model process a batch of sequences in parallel. The main technic lies in creating batches of data and then tranposing targets/output for training/evaluating as the target sequences are sent to decoder in batches. Decoder in models without batching takes one token at a single time stamp, where as decoder with batching takes batch_size times tokens at a single time stamp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Encoder\n",
    "\n",
    "Embedded input sequences (batch_size x max_seq_len_input x emb_dim) are packed (1 x batch_size*max_seq_len_input*emb_dim) before sending to LSTM layer to get rid of pading. The output of LSTM layer is unpacked to get back to padded batches of outputs (batch_size x max_seq_len_input x lstm_size). Both packing and unpacking, along with LSTM, use <code>batch_first = True</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchEncoder(\n",
       "  (emb): Embedding(27, 64, padding_idx=0)\n",
       "  (lstm): LSTM(64, 128, batch_first=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified to process batch of inputs in parallel\n",
    "class BatchEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.1):\n",
    "        super(BatchEncoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim,padding_idx=PAD_IX)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_seqs, input_lengths):\n",
    "        \n",
    "        # input_seqs shape: # batch_size x max_seq_len_input\n",
    "        embedded = self.emb(input_seqs) # batch_size x max_seq_len_input x emb_dim\n",
    "        embedded = self.drop(embedded) # batch_size x max_seq_len_input x emb_dim\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu().numpy(), batch_first=True)\n",
    "        outputs, (h_n, c_n) = self.lstm(packed)\n",
    "\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)  # unpack (back to padded)\n",
    "        # outputs shape:  batch_size x max_seq_len_input x lstm_size\n",
    "        \n",
    "        if self.z_index == 1:\n",
    "            return h_n[0], c_n[0] # (max_seq_len_input x lstm_dim)\n",
    "        else:\n",
    "            return outputs # batch_size x max_seq_len_input x lstm_size\n",
    "\n",
    "batchencoder = BatchEncoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=1)\n",
    "batchencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoders\n",
    "\n",
    "Two decoders were implemented: simple batched decoder and batched decoder with attentiom. Major contribution of this work over baseline example:\n",
    "<ul>\n",
    "    <li>Both the decoder implments batch processing of input for training and prediction. </li>\n",
    "    <li>They also implements variable 'Teacher Forcing' (the baseline model has 100% teacher forcing).</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified to process batch of inputs in parallel\n",
    "class BatchDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, dropout=0.5, teacher_forcing_ratio=1.0):\n",
    "        super(BatchDecoder, self).__init__()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim,padding_idx=PAD_IX) #ignore pad tokens \n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.clf = nn.Linear(lstm_size, vocab_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\", ignore_index = PAD_IX) #ingore pads for loss calclation\n",
    "        \n",
    "    def forward(self, state, targets, target_lengths, curr_tokens):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # tragets shape: max_seq_len_target x batch_size \n",
    "        # target_lenghts shape: batch_size\n",
    "        # curr_tokens shape: batch_size \n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(targets.size(0)): # check i-th seq of all seqs in the batch\n",
    "            inp = curr_tokens\n",
    "\n",
    "            emb = self.emb(inp) #batch_size x emb_dim\n",
    "            emb = self.drop(emb) #batch_size x emb_dim\n",
    "            \n",
    "            state = self.lstm(emb, state) #batch_size x lstm_size\n",
    "            q_i, _ = state #batch_size x lstm_size\n",
    "            q_i = self.drop(q_i) #batch_size x lstm_size\n",
    "            scores = self.clf(q_i) # batch_size x tgt_vocab_len\n",
    "            \n",
    "            # loss ignores pads at tokens for loss calculation\n",
    "            loss += self.objective(scores, targets[i]) #batch_size\n",
    "            \n",
    "            # checks if random probability is lower than teacher forcing ration\n",
    "            # if teacher_forcing_ratio=1.0, 100% teacher forcing is applied\n",
    "            if random.random() < self.teacher_forcing_ratio: \n",
    "                curr_tokens = targets[i]  # Ground truth as next token\n",
    "            else:\n",
    "                soft_scores = torch.softmax(scores, dim=1)\n",
    "                curr_tokens = torch.argmax(scores, dim=1) # predicted output as next token\n",
    "\n",
    "        return loss.sum()/target_lengths.sum() # sum (loss per batch) / (length of each seq in the batch)\n",
    "    \n",
    "    def predict(self, state, curr_tokens, maxlen):\n",
    "        preds = [] # list to append predictions\n",
    "        # curr_tokens shape: batch_size\n",
    "\n",
    "        # loop through for predict batch_size tokens at each time step for batch_size seuquences \n",
    "        # maxlen is max_seq_len_target\n",
    "        for i in range(maxlen):\n",
    "            inp = curr_tokens #batch_size x emb_dim\n",
    "            emb = self.emb(inp) #batch_size x emb_dim\n",
    "            state = self.lstm(emb, state) #batch_size x lstm_size\n",
    "            h_i, _ = state #batch_size x lstm_size\n",
    "            scores = self.clf(h_i) # batch_size x tgt_vocab_len\n",
    "            scores = torch.softmax(scores, dim=1) # batch_size x tgt_vocab_len\n",
    "            pred = torch.argmax(scores, dim=1) # batch_size\n",
    "            curr_tokens = pred # batch_size\n",
    "            preds.append(pred)\n",
    "            \n",
    "        # transpose the predicted output to batch_size x max_seq_len_target\n",
    "        # use len(target sequence) or <stop> token to remove extra predicted words\n",
    "        finalpreds =  []\n",
    "        for j in range(len(preds[0])): # loop throgh batch\n",
    "            temp = []\n",
    "            for i in range(len(preds)): # for 0 to len(sequences)\n",
    "                l = preds[i].cpu().data.numpy() \n",
    "                if(l[j] != STOP_IX): # if not a <stop> token\n",
    "                    temp.append(l[j]) # append to sequence\n",
    "            finalpreds.append(temp) # append to sequence to batch     \n",
    "        return finalpreds, preds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no changes from the baseline code\n",
    "class BatchAttention(nn.Module):\n",
    "    def __init__(self, input_dim, attn_dim):\n",
    "        super(BatchAttention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, attn_dim)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs):\n",
    "        seqlen = enc_outs.size(1)\n",
    "        \n",
    "        repeat_h = dec_hidden.unsqueeze(1)  # make room to repeat on seqlen dim\n",
    "        repeat_h = repeat_h.repeat(1, seqlen, 1)  # (1, seqlen, hidden)\n",
    "\n",
    "        concat_h = torch.cat((enc_outs, repeat_h), dim=2) # (1, seqlen, hidden*2)\n",
    "        \n",
    "        scores = self.v(torch.tanh(self.W(concat_h))) # (1, seqlen, 1)\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        weighted = enc_outs * probs # (1, seqlen, hidden)\n",
    "        \n",
    "        context = torch.sum(weighted, dim=1, keepdim=False) # (1, hidden)\n",
    "        combined = torch.cat((dec_hidden, context), dim=1)  # (1, hidden*2)\n",
    "        \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified to process batch of inputs in parallel\n",
    "class BatchAttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, attn_size, dropout=0.5, teacher_forcing_ratio=1.0):\n",
    "        super(BatchAttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_IX)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.attn = BatchAttention(lstm_size * 2, attn_size)\n",
    "        self.clf = nn.Linear(lstm_size * 2, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\", ignore_index = PAD_IX)\n",
    "    \n",
    "    \n",
    "    def init_state(self, device, batch_size):\n",
    "        # no change in code for init_state\n",
    "        h_0 = torch.zeros(batch_size, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        c_0 = torch.zeros(batch_size, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        return h_0, c_0\n",
    "        \n",
    "    def forward(self, enc_outs, targets, target_lengths, curr_tokens):\n",
    "        loss = 0\n",
    "        \n",
    "        # tragets shape: max_seq_len_target x batch_size \n",
    "        # target_lenghts shape: batch_size\n",
    "        # curr_tokens shape: batch_size \n",
    "        \n",
    "        batch_size = targets.size(1)\n",
    "        max_seq_len = targets.size(0)\n",
    "        \n",
    "        # intilialize the state to 0\n",
    "        state = self.init_state(device, batch_size)\n",
    "        \n",
    "        for i in range(max_seq_len):# check i-th seq of all seqs in the batch\n",
    "            \n",
    "            inp = curr_tokens\n",
    "            emb = self.emb(inp) # batch_size x emb_dim\n",
    "            emb = self.drop(emb) #batch_size x emb_dim\n",
    "            \n",
    "            state = self.lstm(emb, state) # batch_size x lstm_size\n",
    "            q_i, _ = state  # batch_size x lstm_size\n",
    "            q_i = self.drop(q_i) # batch_size x lstm_size\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs) # batch_size x lstm_size*2\n",
    "            \n",
    "            scores = self.clf(combined) # batch_size x tgt_vocab_size\n",
    "            loss += self.objective(scores, targets[i]) # batch_size\n",
    "            \n",
    "            # checks if random probability is lower than teacher forcing ration\n",
    "            # if teacher_forcing_ratio=1.0, 100% teacher forcing is applied\n",
    "            if random.random() < self.teacher_forcing_ratio: # Teacher Forcing Ration = 0.5\n",
    "                curr_tokens = targets[i]  # Ground truth as next token\n",
    "            else:\n",
    "                soft_scores = torch.softmax(scores, dim=1)\n",
    "                curr_tokens = torch.argmax(scores, dim=1) # predicted output as next token\n",
    "            \n",
    "        return loss.sum() / target_lengths.sum() # sum (loss per batch) / (length of each seq in the batch)\n",
    "\n",
    "    def predict(self, enc_outs, curr_tokens, maxlen):\n",
    "        preds = [] # list to append predictions\n",
    "        # curr_tokens shape: batch_size\n",
    "\n",
    "        device = enc_outs.device\n",
    "        state = self.init_state(device, len(curr_tokens))\n",
    "        \n",
    "        # loop through for predict batch_size tokens at each time step for batch_size seuquences \n",
    "        # maxlen is max_seq_len_target\n",
    "        \n",
    "        for i in range(maxlen):\n",
    "            inp = curr_tokens #batch_size x emb_dim\n",
    "            emb = self.emb(inp) #batch_size x emb_dim\n",
    "            state = self.lstm(emb, state) #batch_size x lstm_size\n",
    "            q_i, _ = state\n",
    "            combined = self.attn(q_i, enc_outs)  # batch_size x lstm_size*2\n",
    "            scores = self.clf(combined) # batch_size x tgt_vocab_len\n",
    "            scores = torch.softmax(scores, dim=1 ) # batch_size x tgt_vocab_len\n",
    "            pred = torch.argmax(scores, dim=1) # batch_size\n",
    "            curr_tokens = pred # batch_size\n",
    "                \n",
    "            preds.append(pred)\n",
    "        \n",
    "        # transpose the predicted output to batch_size x max_seq_len_target\n",
    "        # use len(target sequence) or <stop> token to remove extra predicted words\n",
    "        \n",
    "        finalpreds =  []\n",
    "        for j in range(len(preds[0])): # loop throgh batch\n",
    "            temp = []\n",
    "            for i in range(len(preds)): # for 0 to len(sequences)\n",
    "                l = preds[i].cpu().data.numpy() \n",
    "                if(l[j] != STOP_IX): # if not a <stop> token\n",
    "                    temp.append(l[j]) # append to sequence\n",
    "            finalpreds.append(temp) # append to sequence to batch     \n",
    "        return finalpreds, preds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZJVWiK16FtI"
   },
   "source": [
    "# Traning\n",
    "\n",
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to track best model at training epochs, modified from RNN tutorial\n",
    "def track_best_model(model_path, encoder, decoder, epoch, best_acc, dev_acc):\n",
    "    if best_acc > dev_acc:\n",
    "        return best_acc, ''\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'acc': dev_acc,\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'decoder': decoder.state_dict()\n",
    "    }\n",
    "    torch.save(state, model_path)\n",
    "    return dev_acc, ' # BEST # '\n",
    "\n",
    "# same as base model\n",
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "def currenttime():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to convert int sequence to char sequence in batches\n",
    "# LIMITATION: order of original input-target pair is not sorted, how for packing & unpacking at batched encoder they were sorted\n",
    "# Hence, output of prediction from decoder is sorted as well, needs to update the fucnctions or adjust accordingly \n",
    "\n",
    "# funciton to convert int sequences to char sequence (string)\n",
    "def itow(tokenlist, vocab, sep=''):\n",
    "    return ''.join(vocab.itos[i] for i in tokenlist)\n",
    "\n",
    "# convert batch predictions of int sequences to char sequence\n",
    "# same batch has sequences with different length\n",
    "def convert(predcitionlist, target_lengths, vocab):\n",
    "    #print(vocab)\n",
    "    target_lengths = target_lengths.data.cpu().numpy()\n",
    "    result = []\n",
    "    for i in range(len(predcitionlist)):\n",
    "        pred = predcitionlist[i][:target_lengths[i]]\n",
    "        \n",
    "        word =\"\"\n",
    "        for token in pred:\n",
    "            if(token==STOP_IX or token==PAD_IX):\n",
    "                break\n",
    "            else:\n",
    "                word += vocab.itos[token]  \n",
    "\n",
    "        result.append(word)\n",
    "        \n",
    "    return result\n",
    "\n",
    "# function to get accurary of a batch of input\n",
    "# LIMITATION: works for batch_size=1 only, discussed before\n",
    "def accuracy(encoder, decoder, input_x,input_y):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "      \n",
    "    start_batch_inputs = torch.LongTensor([START_IX] * len(input_x)).to(device) #start with <start> token x batch_size\n",
    "    input_input_var, input_input_lengths, input_target_var, input_target_lengths = process_batch(input_x,input_y)\n",
    "    \n",
    "    seq_pairs = sorted(zip(input_x,input_y), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_x,input_y = zip(*seq_pairs)\n",
    "    \n",
    "    maxlen = max(input_target_lengths.data.cpu().numpy())    \n",
    "    encoder_state = encoder(input_input_var, input_input_lengths)\n",
    "    predictions, rawpred = decoder.predict(encoder_state, start_batch_inputs, maxlen=maxlen)\n",
    "\n",
    "    # LIMITATION: order of input_y_converted and predic_converted are not same\n",
    "    input_x_converted = convert(input_x, input_input_lengths,tgt_vocab)\n",
    "    input_y_converted = convert(input_y, input_target_lengths,tgt_vocab) \n",
    "    predic_converted = convert(predictions, input_target_lengths,tgt_vocab)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(input_x)):\n",
    "        if input_y_converted[i]==predic_converted[i]:\n",
    "            count+=1      \n",
    "    return (count/len(input_x))\n",
    "\n",
    "# function to overcome accuracy fucntion's limitation to get accurary for batches\n",
    "def batchaccuracy(encoderloaded, decoderloaded, test_x, test_y):\n",
    "    total = 0\n",
    "    for i in range(len(test_x)):\n",
    "        total += accuracy(encoderloaded, decoderloaded, [test_x[i]], [test_y[i]])\n",
    "    return total/len(test_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(encoder, decoder, input_x, input_y, batch_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total_loss = 0 \n",
    "    for i in range(len(input_x)//batch_size):\n",
    "        x = input_x[i*batch_size:(i+1)*batch_size]\n",
    "        y = input_y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "        input_var, input_lengths, target_var, target_lengths = process_batch(x,y)\n",
    "        start_inputs = torch.LongTensor([START_IX] * batch_size).to(device)\n",
    "            \n",
    "        total_loss += decoder(encoder(input_var, input_lengths),target_var, target_lengths, start_inputs)\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5LennHN6FtJ"
   },
   "outputs": [],
   "source": [
    "def train(savefilename, encoder, decoder, train_x, train_y, train_size, valid_x, valid_y,valid_size, batch_size=50, valid_batch_size=50, \n",
    "          epochs=10, print_every=1, plot_every=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    assert(len(train_x) == len(train_y))\n",
    "    assert(len(valid_x) == len(valid_y))\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.99)\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "    best_acc=0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        \n",
    "        encoder.zero_grad(); enc_optim.zero_grad()\n",
    "        decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "        train_x, train_y = shuffle(train_x, train_y)\n",
    "        train_x, train_y = train_x[:train_size], train_y[:train_size]\n",
    "        \n",
    "        epoch_train_loss = 0\n",
    "        epoch_valid_loss = 0\n",
    "        \n",
    "        val_acc = 0\n",
    "        epoch_track =''\n",
    "        \n",
    "        for i in range(len(train_x)//batch_size):\n",
    "            x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            y = train_y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            input_var, input_lengths, target_var, target_lengths = process_batch(x,y)\n",
    "            \n",
    "            start_inputs = torch.LongTensor([START_IX] * batch_size).to(device)\n",
    "            \n",
    "            batch_loss = decoder(encoder(input_var, input_lengths),target_var, target_lengths, start_inputs)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "            encoder.zero_grad(); enc_optim.zero_grad()\n",
    "            decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "            epoch_train_loss += batch_loss.item()\n",
    "            #batch_loss = 0\n",
    "            \n",
    "            \n",
    "            del x, y, input_var, input_lengths, target_var, target_lengths \n",
    "    \n",
    "\n",
    "        #VALIDATION\n",
    "        \n",
    "        encoder.train(False)\n",
    "        decoder.train(False)\n",
    "        \n",
    "        \n",
    "        valid_x, valid_y = shuffle(valid_x, valid_y)\n",
    "        valid_x, valid_y =valid_x[:valid_size], valid_y[:valid_size] \n",
    "        \n",
    "        epoch_train_loss = epoch_train_loss / (len(train_x)//batch_size)\n",
    "        epoch_valid_loss = evaluation(encoder, decoder, valid_x, valid_y, valid_batch_size) / (len(valid_x)//valid_batch_size)\n",
    "  \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            accuracy_total_valid = 0\n",
    "            for v in range(len(valid_x)//valid_batch_size):\n",
    "                vx = valid_x[v*valid_batch_size:(v+1)*valid_batch_size]\n",
    "                vy = valid_y[v*valid_batch_size:(v+1)*valid_batch_size]\n",
    "                val_input_batches, val_input_lengths, val_target_batches, val_target_lengths= process_batch(valid_x, valid_y)\n",
    "                accuracy_total_valid += batchaccuracy(encoder, decoder, vx,vy)\n",
    "            val_acc = accuracy_total_valid / (len(valid_x)//valid_batch_size)\n",
    "\n",
    "            #tracks and saves the best model\n",
    "            best_acc, epoch_track = track_best_model(savefilename+'-BEST.pt', encoder, decoder , epoch, best_acc, val_acc)\n",
    "            \n",
    "            print(f\"**** Epoch {epoch} - Train Loss: {epoch_train_loss:.6f} - Valid Loss: {epoch_valid_loss:.6f} - Validaiton Accuracy: {val_acc:.4f} **** \"+\\\n",
    "                  currenttime()+ \" \"+epoch_track )     \n",
    "\n",
    "    return encoder, decoder, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20000\n",
    "valid_size = 5000\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "attention = True\n",
    "print_every = 1\n",
    "teacher_forcing_ratio=1.0\n",
    "attn_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-1 Definitions  (No Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchEncoder(\n",
      "  (emb): Embedding(27, 64, padding_idx=0)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "BatchDecoder(\n",
      "  (emb): Embedding(29, 64, padding_idx=0)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (clf): Linear(in_features=128, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#encoder decoder without attention\n",
    "encoder1 = BatchEncoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=1)\n",
    "decoder1 = BatchDecoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128)\n",
    "\n",
    "#encoder decoder with attention\n",
    "#encoder = BatchEncoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=0)\n",
    "#decoder = BatchAttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128, attn_size=attn_size, teacher_forcing_ratio = teacher_forcing_ratio)\n",
    "\n",
    "print(encoder1)\n",
    "print(decoder1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Train the Model-1 (No Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Epoch 1 - Train Loss: 2.410730 - Valid Loss: 1.930187 - Validaiton Accuracy: 0.0000 **** 20200410_0744  # BEST # \n",
      "**** Epoch 2 - Train Loss: 1.658312 - Valid Loss: 1.080642 - Validaiton Accuracy: 0.0690 **** 20200410_0754  # BEST # \n",
      "**** Epoch 3 - Train Loss: 1.055140 - Valid Loss: 0.501405 - Validaiton Accuracy: 0.3400 **** 20200410_0804  # BEST # \n",
      "**** Epoch 4 - Train Loss: 0.635189 - Valid Loss: 0.215385 - Validaiton Accuracy: 0.6684 **** 20200410_0813  # BEST # \n",
      "**** Epoch 5 - Train Loss: 0.365940 - Valid Loss: 0.086178 - Validaiton Accuracy: 0.8646 **** 20200410_0823  # BEST # \n",
      "**** Epoch 6 - Train Loss: 0.220170 - Valid Loss: 0.036451 - Validaiton Accuracy: 0.9448 **** 20200410_0833  # BEST # \n",
      "**** Epoch 7 - Train Loss: 0.145917 - Valid Loss: 0.024169 - Validaiton Accuracy: 0.9608 **** 20200410_0843  # BEST # \n",
      "**** Epoch 8 - Train Loss: 0.102736 - Valid Loss: 0.012598 - Validaiton Accuracy: 0.9812 **** 20200410_0853  # BEST # \n",
      "**** Epoch 9 - Train Loss: 0.080204 - Valid Loss: 0.011678 - Validaiton Accuracy: 0.9802 **** 20200410_0903 \n",
      "**** Epoch 10 - Train Loss: 0.063039 - Valid Loss: 0.006561 - Validaiton Accuracy: 0.9888 **** 20200410_0912  # BEST # \n"
     ]
    }
   ],
   "source": [
    "savedmodelpath = 'model-1.pt'\n",
    "# check if model exists, then load\n",
    "if path.exists(savedmodelpath):\n",
    "    state_dict = torch.load(savedmodelpath)\n",
    "    encoder1.load_state_dict(state_dict['encoder'])\n",
    "    decoder1.load_state_dict(state_dict['decoder'])\n",
    "# if model doesn't exiting train the model\n",
    "else:\n",
    "    savefilename = 'model_no-attn-batched-'+currenttime()\n",
    "    encoder1, decoder1, train_losses1,valid_losses1 = train(savefilename, encoder1, decoder1, train_x, train_y, train_size, valid_x, \n",
    "                         valid_y,valid_size, batch_size=batch_size, valid_batch_size=batch_size, epochs=epochs, print_every=1, plot_every=1)\n",
    "    torch.save({'encoder': encoder1.state_dict(),'decoder': decoder1.state_dict()}, savefilename+'-final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchEncoder(\n",
      "  (emb): Embedding(27, 64, padding_idx=0)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "BatchAttentionDecoder(\n",
      "  (emb): Embedding(29, 64, padding_idx=0)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (attn): BatchAttention(\n",
      "    (W): Linear(in_features=256, out_features=100, bias=True)\n",
      "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
      "  )\n",
      "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#encoder decoder without attention\n",
    "#encoder = BatchEncoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=1)\n",
    "#decoder = BatchDecoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128)\n",
    "#encoder decoder with attention\n",
    "encoder2 = BatchEncoder(vocab_size=len(src_vocab), emb_dim=64, lstm_size=128, z_type=0)\n",
    "decoder2 = BatchAttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, lstm_size=128, attn_size=attn_size, teacher_forcing_ratio = teacher_forcing_ratio)\n",
    "\n",
    "print(encoder2)\n",
    "print(decoder2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Epoch 1 - Train Loss: 2.233419 - Valid Loss: 1.311370 - Validaiton Accuracy: 0.0166 **** 20200410_0923  # BEST # \n",
      "**** Epoch 2 - Train Loss: 0.935489 - Valid Loss: 0.308623 - Validaiton Accuracy: 0.5738 **** 20200410_0934  # BEST # \n",
      "**** Epoch 3 - Train Loss: 0.340313 - Valid Loss: 0.073064 - Validaiton Accuracy: 0.8882 **** 20200410_0945  # BEST # \n",
      "**** Epoch 4 - Train Loss: 0.171135 - Valid Loss: 0.040715 - Validaiton Accuracy: 0.9274 **** 20200410_0956  # BEST # \n",
      "**** Epoch 5 - Train Loss: 0.116868 - Valid Loss: 0.024313 - Validaiton Accuracy: 0.9550 **** 20200410_1007  # BEST # \n",
      "**** Epoch 6 - Train Loss: 0.089508 - Valid Loss: 0.016572 - Validaiton Accuracy: 0.9684 **** 20200410_1017  # BEST # \n",
      "**** Epoch 7 - Train Loss: 0.066823 - Valid Loss: 0.011105 - Validaiton Accuracy: 0.9736 **** 20200410_1028  # BEST # \n",
      "**** Epoch 8 - Train Loss: 0.058287 - Valid Loss: 0.011745 - Validaiton Accuracy: 0.9654 **** 20200410_1039 \n",
      "**** Epoch 9 - Train Loss: 0.051461 - Valid Loss: 0.009020 - Validaiton Accuracy: 0.9714 **** 20200410_1050 \n",
      "**** Epoch 10 - Train Loss: 0.044127 - Valid Loss: 0.007839 - Validaiton Accuracy: 0.9766 **** 20200410_1101  # BEST # \n"
     ]
    }
   ],
   "source": [
    "savedmodelpath = 'model.pt'\n",
    "# check if model exists, then load\n",
    "if path.exists(savedmodelpath):\n",
    "    state_dict = torch.load(savedmodelpath)\n",
    "    encoder2.load_state_dict(state_dict['encoder'])\n",
    "    decoder2.load_state_dict(state_dict['decoder'])\n",
    "# if model doesn't exiting train the model\n",
    "else:\n",
    "    savefilename = 'model_attn-batched-'+currenttime()\n",
    "    encoder2, decoder2, train_losses2,valid_losses2 = train(savefilename, encoder2, decoder2, train_x, train_y, train_size, valid_x, \n",
    "                         valid_y,valid_size, batch_size=batch_size, valid_batch_size=batch_size, epochs=epochs, print_every=1, plot_every=1)\n",
    "    torch.save({'encoder': encoder2.state_dict(),'decoder': decoder2.state_dict()}, savefilename+'-final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(epochs)\n",
    "\n",
    "plt.plot(x, train_losses1, label=\"Model-1: Training Loss\")\n",
    "plt.plot(x, valid_losses1, label=\"Model-1: Validation Loss\")\n",
    "plt.plot(x, train_losses2, label=\"Model-2: Training Loss (Attn)\")\n",
    "plt.plot(x, valid_losses2, label=\"Model-2: Validation Loss (Attn)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.txt', delimiter='\\t', header=None, usecols=[0,1])\n",
    "test_inp, test_out = test_data[0], test_data[1]\n",
    "test_x = map_many_elems(test_inp, src_vocab.stoi, istarget=False)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi, istarget=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-1 (No Attention) Test Accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "print(\"Model-1 (No Attention) Test Accuracy:\", batchaccuracy(encoder1, decoder1, test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-2 (Attention) Test Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "print(\"Model-2 (Attention) Test Accuracy:\", batchaccuracy(encoder2, decoder2, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, different performance metrics are analyzed for following two designed models. \n",
    "<br/> <b>Model-1: Batched Sequence-to-Sequence</b>\n",
    "<br/> <b>Model-2: Batched Sequence-to-Sequence with Attention</b>\n",
    "\n",
    "Models were tested with different batch size, epochs and teacher forcing ratio on GPU (Colab) and CPU (PC).\n",
    "\n",
    "<b>GPU vs CPU Performance.</b> As expected, system with GPU (Colab) performed almost three times faster to achieve similar validation accurary while keeping other parameters same.\n",
    "\n",
    "|   System  | Model-Type | Epoch-5 | Teacher-Forcing | BatchSize | Avg Time (Minutes) | Validation Accuracy |\n",
    "|:---------:|------------|--------:|----------------:|----------:|-------------------:|--------------------:|\n",
    "| GPU-Colab | Attn       |       5 |               1 |        10 |                  7 |              97.12% |\n",
    "| CPU       | Attn       |       5 |               1 |        10 |                 20 |              97.64% |\n",
    "\n",
    "<b>Batch Size.</b> On the same system (example Colab GPU), for constant teacher forcing ratio increase Batch Size takes lesser time for each epoch but, validation accuracy was lower after same number of epochs. The training data sets were not sorted. As a result batches required random padding. Training data could be sorted ('bucketing' technique) to minimize the padded and then batches could be randomly selected for training. More padding required the batches to run longer for the largest sequence in the batch.\n",
    "\n",
    "|   System  | Model-Type | Epoch-5 | Teacher Forcing | Batch Size | Avg Time (Minutes) | Validation Accuracy |\n",
    "|:---------:|------------|:-------:|:---------------:|:----------:|:------------------:|:-------------------:|\n",
    "| GPU-Colab | Attn       |    5    |        1        |      5     |         16         |        99.10%       |\n",
    "| GPU-Colab | Attn       |    5    |        1        |     10     |          7         |        97.12%       |\n",
    "| GPU-Colab | Attn       |    5    |        1        |     25     |          4         |        67.30%       |\n",
    "\n",
    "<b>Teacher Forcing.</b> Models had slower convergence when teacher forcing ratio was set to 50%. Lower teacher forcing ratio is expected to slowdown the convergence and cause instability. Without teacher forcing, sequence prediction models use the output from the last time step y(t-1) as input for the model at the current time step X(t).  Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network.\n",
    "\n",
    "| System | Model-Type | Epoch-5 | Teacher-Forcing | BatchSize | Avg Time (Minutes) | Validation Accuracy |\n",
    "|:------:|------------|--------:|----------------:|----------:|-------------------:|--------------------:|\n",
    "| CPU    | Attn       |       5 |               1 |        10 |                 20 |              97.64% |\n",
    "| CPU    | Attn       |       5 |             0.5 |        10 |                N/R |              91.34% |\n",
    "\n",
    "<b>Attention.</b> Model with Attention works faster and achieves better validation accurary (result shows one sample experiment for comparison, best model was trained with 50 epochs for best accuracy). \n",
    "\n",
    "| System | Model-Type | Epoch-5 | Teacher-Forcing | BatchSize | Avg Time (Minutes) | Validation Accuracy |\n",
    "|:------:|------------|--------:|----------------:|----------:|-------------------:|--------------------:|\n",
    "| CPU    | Attn       |      10 |               1 |        10 |                 10 |              97.70% |\n",
    "| CPU    | No Attn    |      10 |               1 |        10 |                  9 |              93.62% |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I referred to class tutorial [1] and following online examples to implement the models:\n",
    "1. https://github.com/gaguilar/basic_nlp_tutorial/\n",
    "2. https://medium.com/@adam.wearne/seq2seq-with-pytorch-46dc00ff5164\n",
    "3. https://github.com/RRisto/seq2seq\n",
    "4. https://github.com/howardyclo/pytorch-seq2seq-example\n",
    "5. https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346\n",
    "6. https://towardsdatascience.com/tuned-version-of-seq2seq-tutorial-ddb64db46e2a\n",
    "7. https://github.com/pengyuchen/PyTorch-Batch-Seq2seq\n",
    "8. https://github.com/spro/practical-pytorch/\n",
    "9. https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
